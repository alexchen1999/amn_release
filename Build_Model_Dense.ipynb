{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNzx86fAt34-"
   },
   "source": [
    "# Create and Train ANN\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7zBlwFYjS9Xh",
    "outputId": "aca3b1dd-dd9f-4669-b0e4-da82783eab3e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Result', '.DS_Store', 'Dataset_model', 'Reservoir', 'Build_Mode_Dense.ipynb', 'old', '__pycache__', 'README.md', '.gitignore', 'Build_Dataset.ipynb', 'Build_Model.py', '.ipynb_checkpoints', 'Dataset_input', 'Build_Dataset.py', '.git', 'Build_Model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Run this cell first\n",
    "import os\n",
    "import sys\n",
    "RunningInCOLAB  = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if RunningInCOLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    DIRECTORY = '/content/drive/My Drive/AMN-Colab/'\n",
    "    sys.path.append('/content/drive/My Drive/amn')\n",
    "    ! pip install cobra\n",
    "    ! pip install silence_tensorflow\n",
    "else:\n",
    "    DIRECTORY = './'\n",
    "print(os.listdir(DIRECTORY))\n",
    "    \n",
    "from Build_Model import *\n",
    "\n",
    "def printout(filename, Stats, model, time): \n",
    "    # printing Stats\n",
    "    if Stats == None:\n",
    "        print('Stats for %s failed CPU-time %.4f' % (filename, time))\n",
    "        return\n",
    "    print('Stats for %s CPU-time %.4f' % (filename, time))\n",
    "    print('R2 = %.4f (+/- %.4f) Constraint = %.4f (+/- %.4f)' % \\\n",
    "          (Stats.train_objective[0], Stats.train_objective[1],\n",
    "           Stats.train_loss[0], Stats.train_loss[1]))\n",
    "    print('Q2 = %.4f (+/- %.4f) Constraint = %.4f (+/- %.4f)' % \\\n",
    "          (Stats.test_objective[0], Stats.test_objective[1],\n",
    "           Stats.test_loss[0], Stats.test_loss[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLJUJ4kCxjf8",
    "outputId": "aeca8606-f76b-4a28-9efb-bc0297ccbde9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 154 ['PFK'] ----------------------------------------------------------------------\n",
      "nbr parameters: 1101\n"
     ]
    }
   ],
   "source": [
    "# Create, train and evaluate ANN models with FBA simulated training set for E. coli core\n",
    "\n",
    "# What you can change \n",
    "seed = 10\n",
    "np.random.seed(seed=seed)  \n",
    "trainname = 'e_coli_core_UB_10000'\n",
    "size = 1000\n",
    "# End of What you can change\n",
    "\n",
    "# Load training set\n",
    "trainingfile = DIRECTORY+'Dataset_model/'+trainname\n",
    "cobramodel = cobra.io.read_sbml_model(trainingfile+'.xml')\n",
    "OBJ = [r.id for r in cobramodel.reactions]\n",
    "parameter = TrainingSet()\n",
    "parameter.load(trainingfile)\n",
    "\n",
    "# Set all pred fluxes to zero\n",
    "Y, zeros = {}, 0 * parameter.Y \n",
    "for i in range(len(OBJ)):\n",
    "    Y[i] = zeros[:,0] # zero vector\n",
    "\n",
    "# train for different flux outputs \n",
    "# when size > len(OBJ) all flux are taken one after another \n",
    "# otherwise (small trainig sets) output fluxes are chosen at random\n",
    "size = size if size < len(OBJ) else len(OBJ)\n",
    "for k in range(size):\n",
    "    \n",
    "    # choose an output flux at random when size < range(len(OBJ))\n",
    "    i = k if size == len(OBJ) else np.random.randint(0,high=len(OBJ))\n",
    "    objective = [OBJ[i]]\n",
    "    \n",
    "    # create model\n",
    "    print(k+1,'/',size, objective,'----------------------------------------------------------------------')\n",
    "    if np.sum(parameter.Y[:,i]) == 0:\n",
    "        continue # Skip all parameter.Y beeing at zero\n",
    "    model = Neural_Model(trainingfile = trainingfile, \n",
    "                         objective=objective, \n",
    "                         model_type = 'ANN_Dense',\n",
    "                         n_hidden = 1, hidden_dim = 50, \n",
    "                         epochs = 500, xfold = 5)\n",
    "        \n",
    "    # Train and evaluate\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        reservoir, pred, stats, _ = train_evaluate_model(model, verbose=False)\n",
    "    except:\n",
    "        reservoir, pred, stats, _ = None, zeros, None, None\n",
    "    delta_time = time.time() - start_time\n",
    "\n",
    "    # Printing cross-validation results\n",
    "    printout(objective, stats, reservoir, delta_time)\n",
    "    Y[i] = pred[:,0]\n",
    "\n",
    "# Collate all predicted Y and get stats and constraints\n",
    "Y = np.transpose(np.asarray(list(Y.values())))\n",
    "print(Y.shape, parameter.Y.shape)\n",
    "print('Q2=', r2_score(parameter.Y, Y, multioutput='variance_weighted'))\n",
    "X = tf.convert_to_tensor(np.float32(model.X)) # Loss computed of tf tensors\n",
    "Y = tf.convert_to_tensor(np.float32(Y))\n",
    "L2, _ = Loss_SV(Y, model.S)\n",
    "L2 = np.mean(L2.numpy())\n",
    "print('Loss_SV =', L2)\n",
    "L3, _ = Loss_Vin(Y, model.Pin, X, 'UB')\n",
    "L3 = np.mean(L3.numpy())\n",
    "print('Loss_Vin =', L3)\n",
    "L = (L2+L3)/2\n",
    "print('Constraints =', L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Build_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
